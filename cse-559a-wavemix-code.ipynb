{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final project for Computer Vision: WaveMix","metadata":{}},{"cell_type":"markdown","source":"Tung Lun (Tony) NGOK","metadata":{}},{"cell_type":"code","source":"!pip install einops\n!pip install torchsummary\n!pip install dualopt","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:00:34.288192Z","iopub.execute_input":"2024-05-05T23:00:34.288874Z","iopub.status.idle":"2024-05-05T23:01:11.985209Z","shell.execute_reply.started":"2024-05-05T23:00:34.288839Z","shell.execute_reply":"2024-05-05T23:01:11.984174Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nCollecting dualopt\n  Downloading dualopt-0.1.8-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from dualopt) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from dualopt) (0.16.2)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (from dualopt) (1.3.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from dualopt) (1.26.4)\nCollecting lion-pytorch (from dualopt)\n  Downloading lion_pytorch-0.1.4-py3-none-any.whl.metadata (618 bytes)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->dualopt) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->dualopt) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->dualopt) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->dualopt) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->dualopt) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->dualopt) (2024.2.0)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics->dualopt) (21.3)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics->dualopt) (0.11.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->dualopt) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->dualopt) (9.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics->dualopt) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics->dualopt) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->dualopt) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->dualopt) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->dualopt) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->dualopt) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->dualopt) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->dualopt) (1.3.0)\nDownloading dualopt-0.1.8-py3-none-any.whl (5.0 kB)\nDownloading lion_pytorch-0.1.4-py3-none-any.whl (4.3 kB)\nInstalling collected packages: lion-pytorch, dualopt\nSuccessfully installed dualopt-0.1.8 lion-pytorch-0.1.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.autograd import Function\nimport torch.nn as nn\nimport pywt\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchsummary import summary\n\n# https://pypi.org/project/dualopt/\nimport dualopt\nfrom dualopt import classification, post_train\n\nfrom PIL import Image\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport os, glob\n\ntorch.backends.cudnn.benchmarks = True\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:11.987330Z","iopub.execute_input":"2024-05-05T23:01:11.987652Z","iopub.status.idle":"2024-05-05T23:01:20.675940Z","shell.execute_reply.started":"2024-05-05T23:01:11.987619Z","shell.execute_reply":"2024-05-05T23:01:20.675123Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# use GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(torch.cuda.get_device_properties(device))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:20.677090Z","iopub.execute_input":"2024-05-05T23:01:20.677587Z","iopub.status.idle":"2024-05-05T23:01:20.723882Z","shell.execute_reply.started":"2024-05-05T23:01:20.677541Z","shell.execute_reply":"2024-05-05T23:01:20.722817Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16276MB, multi_processor_count=56)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading the Tiny ImageNet dataset","metadata":{}},{"cell_type":"code","source":"!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:20.725641Z","iopub.execute_input":"2024-05-05T23:01:20.725962Z","iopub.status.idle":"2024-05-05T23:01:33.328854Z","shell.execute_reply.started":"2024-05-05T23:01:20.725934Z","shell.execute_reply":"2024-05-05T23:01:33.327682Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-05-05 23:01:21--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\nResolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\nConnecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n--2024-05-05 23:01:21--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\nConnecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 248100043 (237M) [application/zip]\nSaving to: 'tiny-imagenet-200.zip'\n\ntiny-imagenet-200.z 100%[===================>] 236.61M  19.8MB/s    in 11s     \n\n2024-05-05 23:01:33 (20.8 MB/s) - 'tiny-imagenet-200.zip' saved [248100043/248100043]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip -q tiny-imagenet-200.zip\n!rm tiny-imagenet-200.zip","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:33.332512Z","iopub.execute_input":"2024-05-05T23:01:33.332875Z","iopub.status.idle":"2024-05-05T23:01:43.852304Z","shell.execute_reply.started":"2024-05-05T23:01:33.332842Z","shell.execute_reply":"2024-05-05T23:01:43.851013Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"id_dict = {}\nfor i, line in enumerate(open('/kaggle/working/tiny-imagenet-200/wnids.txt', 'r')):\n    id_dict[line.replace('\\n', '')] = i","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:43.853667Z","iopub.execute_input":"2024-05-05T23:01:43.853946Z","iopub.status.idle":"2024-05-05T23:01:43.859881Z","shell.execute_reply.started":"2024-05-05T23:01:43.853916Z","shell.execute_reply":"2024-05-05T23:01:43.858986Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# https://github.com/pranavphoenix/TinyImageNetLoader/blob/main/tinyimagenetloader.py\n# dataset loader provided by the author\n\nclass TrainTinyImageNetDataset(Dataset):\n    def __init__(self, id, transform=None):\n        self.filenames = glob.glob(\"/kaggle/working/tiny-imagenet-200/train/*/*/*.JPEG\")\n        self.transform = transform\n        self.id_dict = id\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        image = Image.open(img_path)\n\n        if image.mode == \"L\":\n          image = image.convert('RGB')\n        label = self.id_dict[img_path.split('/')[5]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:43.861068Z","iopub.execute_input":"2024-05-05T23:01:43.861375Z","iopub.status.idle":"2024-05-05T23:01:43.871864Z","shell.execute_reply.started":"2024-05-05T23:01:43.861352Z","shell.execute_reply":"2024-05-05T23:01:43.871017Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TestTinyImageNetDataset(Dataset):\n    def __init__(self, id, transform=None):\n        self.filenames = glob.glob(\"/kaggle/working/tiny-imagenet-200/val/images/*.JPEG\")\n        self.transform = transform\n        self.id_dict = id\n        self.cls_dic = {}\n        for i, line in enumerate(open('/kaggle/working/tiny-imagenet-200/val/val_annotations.txt', 'r')):\n            a = line.split('\\t')\n            img, cls_id = a[0],a[1]\n            self.cls_dic[img] = self.id_dict[cls_id]\n\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        image = Image.open(img_path)\n        if image.mode == \"L\":\n          image = image.convert('RGB')\n        label = self.cls_dic[img_path.split('/')[-1]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:43.872917Z","iopub.execute_input":"2024-05-05T23:01:43.873481Z","iopub.status.idle":"2024-05-05T23:01:43.883020Z","shell.execute_reply.started":"2024-05-05T23:01:43.873447Z","shell.execute_reply":"2024-05-05T23:01:43.882148Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# transforms\ntransform_train_1 = transforms.Compose(\n        [\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.TrivialAugmentWide(),\n            transforms.ToTensor(),\n     transforms.Normalize((0.4803, 0.4481, 0.3975), (0.2764, 0.2689, 0.2816))])\n\ntransform_train_2 = transforms.Compose(\n        [\n            transforms.ToTensor(),\n     transforms.Normalize((0.4803, 0.4481, 0.3975), (0.2764, 0.2689, 0.2816))])\n\ntransform_test = transforms.Compose(\n        [\n            transforms.ToTensor(),\n     transforms.Normalize((0.4825, 0.4499, 0.3984), (0.2764, 0.2691, 0.2825))])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:43.884413Z","iopub.execute_input":"2024-05-05T23:01:43.884672Z","iopub.status.idle":"2024-05-05T23:01:43.897473Z","shell.execute_reply.started":"2024-05-05T23:01:43.884650Z","shell.execute_reply":"2024-05-05T23:01:43.896716Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"batch_size = 304","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:43.898462Z","iopub.execute_input":"2024-05-05T23:01:43.898750Z","iopub.status.idle":"2024-05-05T23:01:43.907054Z","shell.execute_reply.started":"2024-05-05T23:01:43.898726Z","shell.execute_reply":"2024-05-05T23:01:43.906184Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# dataset\n\ntrainset_1 = TrainTinyImageNetDataset(id=id_dict, transform=transform_train_1)\ntrainloader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)\n\ntrainset_2 = TrainTinyImageNetDataset(id=id_dict, transform=transform_train_2)\ntrainloader_2 = torch.utils.data.DataLoader(trainset_2, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)\n\ntestset = TestTinyImageNetDataset(id=id_dict, transform = transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:43.908083Z","iopub.execute_input":"2024-05-05T23:01:43.908369Z","iopub.status.idle":"2024-05-05T23:01:44.684078Z","shell.execute_reply.started":"2024-05-05T23:01:43.908347Z","shell.execute_reply":"2024-05-05T23:01:44.683109Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Processing the validation images","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:44.685462Z","iopub.execute_input":"2024-05-05T23:01:44.686154Z","iopub.status.idle":"2024-05-05T23:01:44.868590Z","shell.execute_reply.started":"2024-05-05T23:01:44.686098Z","shell.execute_reply":"2024-05-05T23:01:44.867810Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Gaussian blur function\ndef blur_image(image_array, blur_radius=5):\n    blurred_image = cv2.GaussianBlur(image_array, (blur_radius, blur_radius), 0)\n    return blurred_image","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:44.869670Z","iopub.execute_input":"2024-05-05T23:01:44.869950Z","iopub.status.idle":"2024-05-05T23:01:44.874529Z","shell.execute_reply.started":"2024-05-05T23:01:44.869925Z","shell.execute_reply":"2024-05-05T23:01:44.873622Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Noise-adding function\ndef add_gaussian_noise(image_tensor, mean=0., std=0.1):\n    noise = torch.randn(image_tensor.size()) * std + mean\n    noisy_image = image_tensor + noise\n    noisy_image = torch.clamp(noisy_image, 0., 1.)\n    return noisy_image","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:44.879389Z","iopub.execute_input":"2024-05-05T23:01:44.879666Z","iopub.status.idle":"2024-05-05T23:01:44.884999Z","shell.execute_reply.started":"2024-05-05T23:01:44.879643Z","shell.execute_reply":"2024-05-05T23:01:44.883993Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# process every image\ndef process_images(input_folder_path, output_folder_blur, output_folder_noise, blur_radius=5, mean=0., std=0.1):\n\n    # check output directory\n    os.makedirs(output_folder_blur, exist_ok=True)\n    os.makedirs(output_folder_noise, exist_ok=True)\n\n    transform_to_tensor = transforms.ToTensor()\n    transform_to_pil = transforms.ToPILImage()\n\n    # Iterate every image\n    for filename in os.listdir(input_folder_path):\n        if filename.endswith(\".JPEG\"):\n            image_path = os.path.join(input_folder_path, filename)\n            image = cv2.imread(image_path)\n\n            # Blur the image\n            blurred_image = blur_image(image, blur_radius)\n            cv2.imwrite(os.path.join(output_folder_blur, filename), blurred_image)\n\n            # Adding noise\n            image_pil = Image.open(image_path)\n            image_tensor = transform_to_tensor(image_pil)\n\n            # Save the noised images\n            noisy_image_tensor = add_gaussian_noise(image_tensor, mean, std)\n            noisy_image_pil = transform_to_pil(noisy_image_tensor)\n            noisy_image_pil.save(os.path.join(output_folder_noise, filename))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:44.886149Z","iopub.execute_input":"2024-05-05T23:01:44.886483Z","iopub.status.idle":"2024-05-05T23:01:44.895832Z","shell.execute_reply.started":"2024-05-05T23:01:44.886460Z","shell.execute_reply":"2024-05-05T23:01:44.894946Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# setting path\ninput_folder_path = \"/kaggle/working/tiny-imagenet-200/val/images\"  # folder path of original images\noutput_folder_blur = \"/kaggle/working/tiny-imagenet-200/val_blur/images\"  # folder path of saving blurred image\noutput_folder_noise = \"/kaggle/working/tiny-imagenet-200/val_noise/images\"  # folder path of saving noised image\n\nprocess_images(input_folder_path, output_folder_blur, output_folder_noise)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:01:44.896903Z","iopub.execute_input":"2024-05-05T23:01:44.897185Z","iopub.status.idle":"2024-05-05T23:02:04.315084Z","shell.execute_reply.started":"2024-05-05T23:01:44.897153Z","shell.execute_reply":"2024-05-05T23:02:04.314194Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class TestBlurredTinyImageNetDataset(Dataset):\n    def __init__(self, id, transform=None):\n        self.filenames = glob.glob(\"/kaggle/working/tiny-imagenet-200/val_blur/images/*.JPEG\")\n        self.transform = transform\n        self.id_dict = id\n        self.cls_dic = {}\n        for i, line in enumerate(open('/kaggle/working/tiny-imagenet-200/val/val_annotations.txt', 'r')):\n            a = line.split('\\t')\n            img, cls_id = a[0],a[1]\n            self.cls_dic[img] = self.id_dict[cls_id]\n\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        image = Image.open(img_path)\n        if image.mode == \"L\":\n          image = image.convert('RGB')\n        label = self.cls_dic[img_path.split('/')[-1]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:04.316382Z","iopub.execute_input":"2024-05-05T23:02:04.316743Z","iopub.status.idle":"2024-05-05T23:02:04.325807Z","shell.execute_reply.started":"2024-05-05T23:02:04.316710Z","shell.execute_reply":"2024-05-05T23:02:04.324863Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# blurred dataset\n\ntrainset_blur_1 = TrainTinyImageNetDataset(id=id_dict, transform=transform_train_1)\ntrainloader_blur_1 = torch.utils.data.DataLoader(trainset_1, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)\n\ntrainset_blur_2 = TrainTinyImageNetDataset(id=id_dict, transform=transform_train_2)\ntrainloader_blur_2 = torch.utils.data.DataLoader(trainset_2, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)\n\ntestset_blur = TestBlurredTinyImageNetDataset(id=id_dict, transform=transform_test)\ntestloader_blur = torch.utils.data.DataLoader(testset_blur, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:04.326962Z","iopub.execute_input":"2024-05-05T23:02:04.327397Z","iopub.status.idle":"2024-05-05T23:02:05.102920Z","shell.execute_reply.started":"2024-05-05T23:02:04.327366Z","shell.execute_reply":"2024-05-05T23:02:05.101938Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class TestNoisedTinyImageNetDataset(Dataset):\n    def __init__(self, id, transform=None):\n        self.filenames = glob.glob(\"/kaggle/working/tiny-imagenet-200/val_noise/images/*.JPEG\")\n        self.transform = transform\n        self.id_dict = id\n        self.cls_dic = {}\n        for i, line in enumerate(open('/kaggle/working/tiny-imagenet-200/val/val_annotations.txt', 'r')):\n            a = line.split('\\t')\n            img, cls_id = a[0],a[1]\n            self.cls_dic[img] = self.id_dict[cls_id]\n\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        image = Image.open(img_path)\n        if image.mode == \"L\":\n          image = image.convert('RGB')\n        label = self.cls_dic[img_path.split('/')[-1]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.104107Z","iopub.execute_input":"2024-05-05T23:02:05.104428Z","iopub.status.idle":"2024-05-05T23:02:05.112404Z","shell.execute_reply.started":"2024-05-05T23:02:05.104402Z","shell.execute_reply":"2024-05-05T23:02:05.111459Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# noised dataset\n\ntrainset_noise_1 = TrainTinyImageNetDataset(id=id_dict, transform=transform_train_1)\ntrainloader_noise_1 = torch.utils.data.DataLoader(trainset_1, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)\n\ntrainset_noise_2 = TrainTinyImageNetDataset(id=id_dict, transform=transform_train_2)\ntrainloader_noise_2 = torch.utils.data.DataLoader(trainset_2, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)\n\ntestset_noise = TestNoisedTinyImageNetDataset(id=id_dict, transform=transform_test)\ntestloader_noise = torch.utils.data.DataLoader(testset_noise, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, prefetch_factor=2, persistent_workers=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.113473Z","iopub.execute_input":"2024-05-05T23:02:05.113732Z","iopub.status.idle":"2024-05-05T23:02:05.892354Z","shell.execute_reply.started":"2024-05-05T23:02:05.113710Z","shell.execute_reply":"2024-05-05T23:02:05.891492Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## The model","metadata":{}},{"cell_type":"code","source":"def sfb1d(lo, hi, g0, g1, mode='zero', dim=-1):\n    \"\"\" 1D synthesis filter bank of an image tensor\n    \"\"\"\n    C = lo.shape[1]\n    d = dim % 4\n    # If g0, g1 are not tensors, make them. If they are, then assume that they\n    # are in the right order\n    if not isinstance(g0, torch.Tensor):\n        g0 = torch.tensor(np.copy(np.array(g0).ravel()),\n                          dtype=torch.float, device=lo.device)\n    if not isinstance(g1, torch.Tensor):\n        g1 = torch.tensor(np.copy(np.array(g1).ravel()),\n                          dtype=torch.float, device=lo.device)\n    L = g0.numel()\n    shape = [1,1,1,1]\n    shape[d] = L\n    N = 2*lo.shape[d]\n    # If g aren't in the right shape, make them so\n    if g0.shape != tuple(shape):\n        g0 = g0.reshape(*shape)\n    if g1.shape != tuple(shape):\n        g1 = g1.reshape(*shape)\n\n    s = (2, 1) if d == 2 else (1,2)\n    g0 = torch.cat([g0]*C,dim=0)\n    g1 = torch.cat([g1]*C,dim=0)\n    if mode == 'per' or mode == 'periodization':\n        y = F.conv_transpose2d(lo, g0, stride=s, groups=C) + \\\n            F.conv_transpose2d(hi, g1, stride=s, groups=C)\n        if d == 2:\n            y[:,:,:L-2] = y[:,:,:L-2] + y[:,:,N:N+L-2]\n            y = y[:,:,:N]\n        else:\n            y[:,:,:,:L-2] = y[:,:,:,:L-2] + y[:,:,:,N:N+L-2]\n            y = y[:,:,:,:N]\n        y = roll(y, 1-L//2, dim=dim)\n    else:\n        if mode == 'zero' or mode == 'symmetric' or mode == 'reflect' or \\\n                mode == 'periodic':\n            pad = (L-2, 0) if d == 2 else (0, L-2)\n            y = F.conv_transpose2d(lo, g0, stride=s, padding=pad, groups=C) + \\\n                F.conv_transpose2d(hi, g1, stride=s, padding=pad, groups=C)\n        else:\n            raise ValueError(\"Unkown pad type: {}\".format(mode))\n\n    return y","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.893430Z","iopub.execute_input":"2024-05-05T23:02:05.893709Z","iopub.status.idle":"2024-05-05T23:02:05.917194Z","shell.execute_reply.started":"2024-05-05T23:02:05.893686Z","shell.execute_reply":"2024-05-05T23:02:05.916193Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def reflect(x, minx, maxx):\n    \"\"\"Reflect the values in matrix *x* about the scalar values *minx* and\n    *maxx*.  Hence a vector *x* containing a long linearly increasing series is\n    converted into a waveform which ramps linearly up and down between *minx*\n    and *maxx*.  If *x* contains integers and *minx* and *maxx* are (integers +\n    0.5), the ramps will have repeated max and min samples.\n    .. codeauthor:: Rich Wareham <rjw57@cantab.net>, Aug 2013\n    .. codeauthor:: Nick Kingsbury, Cambridge University, January 1999.\n    \"\"\"\n    x = np.asanyarray(x)\n    rng = maxx - minx\n    rng_by_2 = 2 * rng\n    mod = np.fmod(x - minx, rng_by_2)\n    normed_mod = np.where(mod < 0, mod + rng_by_2, mod)\n    out = np.where(normed_mod >= rng, rng_by_2 - normed_mod, normed_mod) + minx\n    return np.array(out, dtype=x.dtype)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.918361Z","iopub.execute_input":"2024-05-05T23:02:05.918632Z","iopub.status.idle":"2024-05-05T23:02:05.930878Z","shell.execute_reply.started":"2024-05-05T23:02:05.918609Z","shell.execute_reply":"2024-05-05T23:02:05.930063Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def mode_to_int(mode):\n    if mode == 'zero':\n        return 0\n    elif mode == 'symmetric':\n        return 1\n    elif mode == 'per' or mode == 'periodization':\n        return 2\n    elif mode == 'constant':\n        return 3\n    elif mode == 'reflect':\n        return 4\n    elif mode == 'replicate':\n        return 5\n    elif mode == 'periodic':\n        return 6\n    else:\n        raise ValueError(\"Unkown pad type: {}\".format(mode))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.931858Z","iopub.execute_input":"2024-05-05T23:02:05.932117Z","iopub.status.idle":"2024-05-05T23:02:05.943682Z","shell.execute_reply.started":"2024-05-05T23:02:05.932087Z","shell.execute_reply":"2024-05-05T23:02:05.942946Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def int_to_mode(mode):\n    if mode == 0:\n        return 'zero'\n    elif mode == 1:\n        return 'symmetric'\n    elif mode == 2:\n        return 'periodization'\n    elif mode == 3:\n        return 'constant'\n    elif mode == 4:\n        return 'reflect'\n    elif mode == 5:\n        return 'replicate'\n    elif mode == 6:\n        return 'periodic'\n    else:\n        raise ValueError(\"Unkown pad type: {}\".format(mode))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.944557Z","iopub.execute_input":"2024-05-05T23:02:05.944830Z","iopub.status.idle":"2024-05-05T23:02:05.953719Z","shell.execute_reply.started":"2024-05-05T23:02:05.944808Z","shell.execute_reply":"2024-05-05T23:02:05.952762Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def afb1d(x, h0, h1, mode='zero', dim=-1):\n    \"\"\" 1D analysis filter bank (along one dimension only) of an image\n    Inputs:\n        x (tensor): 4D input with the last two dimensions the spatial input\n        h0 (tensor): 4D input for the lowpass filter. Should have shape (1, 1,\n            h, 1) or (1, 1, 1, w)\n        h1 (tensor): 4D input for the highpass filter. Should have shape (1, 1,\n            h, 1) or (1, 1, 1, w)\n        mode (str): padding method\n        dim (int) - dimension of filtering. d=2 is for a vertical filter (called\n            column filtering but filters across the rows). d=3 is for a\n            horizontal filter, (called row filtering but filters across the\n            columns).\n    Returns:\n        lohi: lowpass and highpass subbands concatenated along the channel\n            dimension\n    \"\"\"\n    C = x.shape[1]\n    # Convert the dim to positive\n    d = dim % 4\n    s = (2, 1) if d == 2 else (1, 2)\n    N = x.shape[d]\n    # If h0, h1 are not tensors, make them. If they are, then assume that they\n    # are in the right order\n    if not isinstance(h0, torch.Tensor):\n        h0 = torch.tensor(np.copy(np.array(h0).ravel()[::-1]),\n                          dtype=torch.float, device=x.device)\n    if not isinstance(h1, torch.Tensor):\n        h1 = torch.tensor(np.copy(np.array(h1).ravel()[::-1]),\n                          dtype=torch.float, device=x.device)\n    L = h0.numel()\n    L2 = L // 2\n    shape = [1,1,1,1]\n    shape[d] = L\n    # If h aren't in the right shape, make them so\n    if h0.shape != tuple(shape):\n        h0 = h0.reshape(*shape)\n    if h1.shape != tuple(shape):\n        h1 = h1.reshape(*shape)\n    h = torch.cat([h0, h1] * C, dim=0)\n\n    if mode == 'per' or mode == 'periodization':\n        if x.shape[dim] % 2 == 1:\n            if d == 2:\n                x = torch.cat((x, x[:,:,-1:]), dim=2)\n            else:\n                x = torch.cat((x, x[:,:,:,-1:]), dim=3)\n            N += 1\n        x = roll(x, -L2, dim=d)\n        pad = (L-1, 0) if d == 2 else (0, L-1)\n        lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)\n        N2 = N//2\n        if d == 2:\n            lohi[:,:,:L2] = lohi[:,:,:L2] + lohi[:,:,N2:N2+L2]\n            lohi = lohi[:,:,:N2]\n        else:\n            lohi[:,:,:,:L2] = lohi[:,:,:,:L2] + lohi[:,:,:,N2:N2+L2]\n            lohi = lohi[:,:,:,:N2]\n    else:\n        # Calculate the pad size\n        outsize = pywt.dwt_coeff_len(N, L, mode=mode)\n        p = 2 * (outsize - 1) - N + L\n        if mode == 'zero':\n            # Sadly, pytorch only allows for same padding before and after, if\n            # we need to do more padding after for odd length signals, have to\n            # prepad\n            if p % 2 == 1:\n                pad = (0, 0, 0, 1) if d == 2 else (0, 1, 0, 0)\n                x = F.pad(x, pad)\n            pad = (p//2, 0) if d == 2 else (0, p//2)\n            # Calculate the high and lowpass\n            lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)\n        elif mode == 'symmetric' or mode == 'reflect' or mode == 'periodic':\n            pad = (0, 0, p//2, (p+1)//2) if d == 2 else (p//2, (p+1)//2, 0, 0)\n            x = mypad(x, pad=pad, mode=mode)\n            lohi = F.conv2d(x, h, stride=s, groups=C)\n        else:\n            raise ValueError(\"Unkown pad type: {}\".format(mode))\n\n    return lohi","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.955081Z","iopub.execute_input":"2024-05-05T23:02:05.955419Z","iopub.status.idle":"2024-05-05T23:02:05.974944Z","shell.execute_reply.started":"2024-05-05T23:02:05.955390Z","shell.execute_reply":"2024-05-05T23:02:05.974271Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class AFB2D(Function):\n    \"\"\" Does a single level 2d wavelet decomposition of an input. Does separate\n    row and column filtering by two calls to\n    :py:func:`pytorch_wavelets.dwt.lowlevel.afb1d`\n    Needs to have the tensors in the right form. Because this function defines\n    its own backward pass, saves on memory by not having to save the input\n    tensors.\n    Inputs:\n        x (torch.Tensor): Input to decompose\n        h0_row: row lowpass\n        h1_row: row highpass\n        h0_col: col lowpass\n        h1_col: col highpass\n        mode (int): use mode_to_int to get the int code here\n    We encode the mode as an integer rather than a string as gradcheck causes an\n    error when a string is provided.\n    Returns:\n        y: Tensor of shape (N, C*4, H, W)\n    \"\"\"\n    @staticmethod\n    def forward(ctx, x, h0_row, h1_row, h0_col, h1_col, mode):\n        ctx.save_for_backward(h0_row, h1_row, h0_col, h1_col)\n        ctx.shape = x.shape[-2:]\n        mode = int_to_mode(mode)\n        ctx.mode = mode\n        lohi = afb1d(x, h0_row, h1_row, mode=mode, dim=3)\n        y = afb1d(lohi, h0_col, h1_col, mode=mode, dim=2)\n        s = y.shape\n        y = y.reshape(s[0], -1, 4, s[-2], s[-1])\n        low = y[:,:,0].contiguous()\n        highs = y[:,:,1:].contiguous()\n        return low, highs\n\n    @staticmethod\n    def backward(ctx, low, highs):\n        dx = None\n        if ctx.needs_input_grad[0]:\n            mode = ctx.mode\n            h0_row, h1_row, h0_col, h1_col = ctx.saved_tensors\n            lh, hl, hh = torch.unbind(highs, dim=2)\n            lo = sfb1d(low, lh, h0_col, h1_col, mode=mode, dim=2)\n            hi = sfb1d(hl, hh, h0_col, h1_col, mode=mode, dim=2)\n            dx = sfb1d(lo, hi, h0_row, h1_row, mode=mode, dim=3)\n            if dx.shape[-2] > ctx.shape[-2] and dx.shape[-1] > ctx.shape[-1]:\n                dx = dx[:,:,:ctx.shape[-2], :ctx.shape[-1]]\n            elif dx.shape[-2] > ctx.shape[-2]:\n                dx = dx[:,:,:ctx.shape[-2]]\n            elif dx.shape[-1] > ctx.shape[-1]:\n                dx = dx[:,:,:,:ctx.shape[-1]]\n        return dx, None, None, None, None, None","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.976061Z","iopub.execute_input":"2024-05-05T23:02:05.976770Z","iopub.status.idle":"2024-05-05T23:02:05.991417Z","shell.execute_reply.started":"2024-05-05T23:02:05.976746Z","shell.execute_reply":"2024-05-05T23:02:05.990531Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def prep_filt_afb1d(h0, h1, device=device):\n    \"\"\"\n    Prepares the filters to be of the right form for the afb2d function.  In\n    particular, makes the tensors the right shape. It takes mirror images of\n    them as as afb2d uses conv2d which acts like normal correlation.\n    Inputs:\n        h0 (array-like): low pass column filter bank\n        h1 (array-like): high pass column filter bank\n        device: which device to put the tensors on to\n    Returns:\n        (h0, h1)\n    \"\"\"\n    h0 = np.array(h0[::-1]).ravel()\n    h1 = np.array(h1[::-1]).ravel()\n    t = torch.get_default_dtype()\n    h0 = torch.tensor(h0, device=device, dtype=t).reshape((1, 1, -1))\n    h1 = torch.tensor(h1, device=device, dtype=t).reshape((1, 1, -1))\n    return h0, h1","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:05.992455Z","iopub.execute_input":"2024-05-05T23:02:05.992730Z","iopub.status.idle":"2024-05-05T23:02:06.004068Z","shell.execute_reply.started":"2024-05-05T23:02:05.992698Z","shell.execute_reply":"2024-05-05T23:02:06.003289Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def prep_filt_afb2d(h0_col, h1_col, h0_row=None, h1_row=None, device=device):\n    \"\"\"\n    Prepares the filters to be of the right form for the afb2d function.  In\n    particular, makes the tensors the right shape. It takes mirror images of\n    them as as afb2d uses conv2d which acts like normal correlation.\n    Inputs:\n        h0_col (array-like): low pass column filter bank\n        h1_col (array-like): high pass column filter bank\n        h0_row (array-like): low pass row filter bank. If none, will assume the\n            same as column filter\n        h1_row (array-like): high pass row filter bank. If none, will assume the\n            same as column filter\n        device: which device to put the tensors on to\n    Returns:\n        (h0_col, h1_col, h0_row, h1_row)\n    \"\"\"\n    h0_col, h1_col = prep_filt_afb1d(h0_col, h1_col, device)\n    if h0_row is None:\n        h0_row, h1_col = h0_col, h1_col\n    else:\n        h0_row, h1_row = prep_filt_afb1d(h0_row, h1_row, device)\n\n    h0_col = h0_col.reshape((1, 1, -1, 1))\n    h1_col = h1_col.reshape((1, 1, -1, 1))\n    h0_row = h0_row.reshape((1, 1, 1, -1))\n    h1_row = h1_row.reshape((1, 1, 1, -1))\n    return h0_col, h1_col, h0_row, h1_row","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.005275Z","iopub.execute_input":"2024-05-05T23:02:06.005795Z","iopub.status.idle":"2024-05-05T23:02:06.017922Z","shell.execute_reply.started":"2024-05-05T23:02:06.005770Z","shell.execute_reply":"2024-05-05T23:02:06.017081Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class DWTForward(nn.Module):\n    \"\"\" Performs a 2d DWT Forward decomposition of an image\n    Args:\n        J (int): Number of levels of decomposition\n        wave (str or pywt.Wavelet or tuple(ndarray)): Which wavelet to use.\n            Can be:\n            1) a string to pass to pywt.Wavelet constructor\n            2) a pywt.Wavelet class\n            3) a tuple of numpy arrays, either (h0, h1) or (h0_col, h1_col, h0_row, h1_row)\n        mode (str): 'zero', 'symmetric', 'reflect' or 'periodization'. The\n            padding scheme\n        \"\"\"\n    def __init__(self, J=1, wave='db1', mode='zero'):\n        super().__init__()\n        if isinstance(wave, str):\n            wave = pywt.Wavelet(wave)\n        if isinstance(wave, pywt.Wavelet):\n            h0_col, h1_col = wave.dec_lo, wave.dec_hi\n            h0_row, h1_row = h0_col, h1_col\n        else:\n            if len(wave) == 2:\n                h0_col, h1_col = wave[0], wave[1]\n                h0_row, h1_row = h0_col, h1_col\n            elif len(wave) == 4:\n                h0_col, h1_col = wave[0], wave[1]\n                h0_row, h1_row = wave[2], wave[3]\n\n        # Prepare the filters\n        filts = prep_filt_afb2d(h0_col, h1_col, h0_row, h1_row)\n        self.register_buffer('h0_col', filts[0])\n        self.register_buffer('h1_col', filts[1])\n        self.register_buffer('h0_row', filts[2])\n        self.register_buffer('h1_row', filts[3])\n        self.J = J\n        self.mode = mode\n\n    def forward(self, x):\n        \"\"\" Forward pass of the DWT.\n        Args:\n            x (tensor): Input of shape :math:`(N, C_{in}, H_{in}, W_{in})`\n        Returns:\n            (yl, yh)\n                tuple of lowpass (yl) and bandpass (yh) coefficients.\n                yh is a list of length J with the first entry\n                being the finest scale coefficients. yl has shape\n                :math:`(N, C_{in}, H_{in}', W_{in}')` and yh has shape\n                :math:`list(N, C_{in}, 3, H_{in}'', W_{in}'')`. The new\n                dimension in yh iterates over the LH, HL and HH coefficients.\n        Note:\n            :math:`H_{in}', W_{in}', H_{in}'', W_{in}''` denote the correctly\n            downsampled shapes of the DWT pyramid.\n        \"\"\"\n        yh = []\n        ll = x\n        mode = mode_to_int(self.mode)\n\n        # Do a multilevel transform\n        for j in range(self.J):\n            # Do 1 level of the transform\n            ll, high = AFB2D.apply(\n                ll, self.h0_col, self.h1_col, self.h0_row, self.h1_row, mode)\n            yh.append(high)\n\n        return ll, yh","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.019091Z","iopub.execute_input":"2024-05-05T23:02:06.019403Z","iopub.status.idle":"2024-05-05T23:02:06.031027Z","shell.execute_reply.started":"2024-05-05T23:02:06.019380Z","shell.execute_reply":"2024-05-05T23:02:06.030184Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"xf1 = DWTForward(J=1, mode='zero', wave='db1').to(device)\nxf2 = DWTForward(J=2, mode='zero', wave='db1').to(device)\nxf3 = DWTForward(J=3, mode='zero', wave='db1').to(device)\nxf4 = DWTForward(J=4, mode='zero', wave='db1').to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.031969Z","iopub.execute_input":"2024-05-05T23:02:06.032280Z","iopub.status.idle":"2024-05-05T23:02:06.164452Z","shell.execute_reply.started":"2024-05-05T23:02:06.032238Z","shell.execute_reply":"2024-05-05T23:02:06.163634Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class Level1Waveblock(nn.Module):\n    def __init__(\n        self,\n        *,\n        mult = 2,\n        ff_channel = 16,\n        final_dim = 16,\n        dropout = 0.5,\n    ):\n        super().__init__()\n        \n      \n        self.feedforward = nn.Sequential(\n                nn.Conv2d(final_dim, final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, final_dim, 4, stride=2, padding=1),\n                nn.BatchNorm2d(final_dim)\n            \n            )\n\n        self.reduction = nn.Conv2d(final_dim, int(final_dim/4), 1)\n        \n        \n    def forward(self, x):\n        b, c, h, w = x.shape\n        \n        x = self.reduction(x)\n        \n        Y1, Yh = xf1(x)\n        \n        x = torch.reshape(Yh[0], (b, int(c*3/4), int(h/2), int(w/2)))\n        \n        x = torch.cat((Y1,x), dim = 1)\n        \n        x = self.feedforward(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.165465Z","iopub.execute_input":"2024-05-05T23:02:06.165720Z","iopub.status.idle":"2024-05-05T23:02:06.185402Z","shell.execute_reply.started":"2024-05-05T23:02:06.165697Z","shell.execute_reply":"2024-05-05T23:02:06.184418Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class Level2Waveblock(nn.Module):\n    def __init__(\n        self,\n        *,\n        mult = 2,\n        ff_channel = 16,\n        final_dim = 16,\n        dropout = 0.5,\n    ):\n        super().__init__()\n        \n        self.feedforward1 = nn.Sequential(\n                nn.Conv2d(final_dim + int(final_dim/2), final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, final_dim, 4, stride=2, padding=1),\n                nn.BatchNorm2d(final_dim)         \n            )\n\n        self.feedforward2 = nn.Sequential(\n                nn.Conv2d(final_dim, final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, int(final_dim/2), 4, stride=2, padding=1),\n                nn.BatchNorm2d(int(final_dim/2))            \n            )\n\n        self.reduction = nn.Conv2d(final_dim, int(final_dim/4), 1)\n        \n        \n    def forward(self, x):\n        b, c, h, w = x.shape\n        \n        x = self.reduction(x)\n        \n        Y1, Yh = xf1(x)\n        Y2, Yh = xf2(x)\n\n        \n        x1 = torch.reshape(Yh[0], (b, int(c*3/4), int(h/2), int(w/2)))\n        x2 = torch.reshape(Yh[1], (b, int(c*3/4), int(h/4), int(w/4)))\n        \n        x1 = torch.cat((Y1,x1), dim = 1)\n        x2 = torch.cat((Y2,x2), dim = 1)\n        \n        x2 = self.feedforward2(x2)\n\n        x1 = torch.cat((x1,x2), dim = 1)\n        x = self.feedforward1(x1)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.186404Z","iopub.execute_input":"2024-05-05T23:02:06.186676Z","iopub.status.idle":"2024-05-05T23:02:06.198965Z","shell.execute_reply.started":"2024-05-05T23:02:06.186653Z","shell.execute_reply":"2024-05-05T23:02:06.198176Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class Level3Waveblock(nn.Module):\n    def __init__(\n        self,\n        *,\n        mult = 2,\n        ff_channel = 16,\n        final_dim = 16,\n        dropout = 0.5,\n    ):\n        super().__init__()\n        \n        self.feedforward1 = nn.Sequential(\n                nn.Conv2d(final_dim + int(final_dim/2), final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, final_dim, 4, stride=2, padding=1),\n                nn.BatchNorm2d(final_dim)         \n            )\n\n        self.feedforward2 = nn.Sequential(\n                nn.Conv2d(final_dim + int(final_dim/2), final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, int(final_dim/2), 4, stride=2, padding=1),\n                nn.BatchNorm2d(int(final_dim/2))            \n            )\n\n        self.feedforward3 = nn.Sequential(\n                nn.Conv2d(final_dim, final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, int(final_dim/2), 4, stride=2, padding=1),\n                nn.BatchNorm2d(int(final_dim/2))          \n            )\n\n        self.reduction = nn.Conv2d(final_dim, int(final_dim/4), 1)\n        \n        \n    def forward(self, x):\n        b, c, h, w = x.shape\n        \n        x = self.reduction(x)\n        \n        Y1, Yh = xf1(x)\n        Y2, Yh = xf2(x)\n        Y3, Yh = xf3(x)\n        \n        \n        x1 = torch.reshape(Yh[0], (b, int(c*3/4), int(h/2), int(w/2)))\n        x2 = torch.reshape(Yh[1], (b, int(c*3/4), int(h/4), int(w/4)))\n        x3 = torch.reshape(Yh[2], (b, int(c*3/4), int(h/8), int(w/8)))\n        \n        \n        x1 = torch.cat((Y1,x1), dim = 1)\n        x2 = torch.cat((Y2,x2), dim = 1)\n        x3 = torch.cat((Y3,x3), dim = 1)\n       \n        \n        x3 = self.feedforward3(x3)\n        \n        x2 = torch.cat((x2,x3), dim = 1)\n\n        x2 = self.feedforward2(x2)\n\n        x1 = torch.cat((x1,x2), dim = 1)\n        x = self.feedforward1(x1)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.199905Z","iopub.execute_input":"2024-05-05T23:02:06.200180Z","iopub.status.idle":"2024-05-05T23:02:06.216970Z","shell.execute_reply.started":"2024-05-05T23:02:06.200148Z","shell.execute_reply":"2024-05-05T23:02:06.216196Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class Level4Waveblock(nn.Module):\n    def __init__(\n        self,\n        *,\n        mult = 2,\n        ff_channel = 16,\n        final_dim = 16,\n        dropout = 0.5,\n    ):\n        super().__init__()\n        \n      \n        self.feedforward1 = nn.Sequential(\n                nn.Conv2d(final_dim + int(final_dim/2), final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, final_dim, 4, stride=2, padding=1),\n                nn.BatchNorm2d(final_dim)         \n            )\n\n        self.feedforward2 = nn.Sequential(\n                nn.Conv2d(final_dim + int(final_dim/2), final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, int(final_dim/2), 4, stride=2, padding=1),\n                nn.BatchNorm2d(int(final_dim/2))            \n            )\n\n        self.feedforward3 = nn.Sequential(\n                nn.Conv2d(final_dim+ int(final_dim/2), final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, int(final_dim/2), 4, stride=2, padding=1),\n                nn.BatchNorm2d(int(final_dim/2))          \n            )\n\n        self.feedforward4 = nn.Sequential(\n                nn.Conv2d(final_dim, final_dim*mult,1),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Conv2d(final_dim*mult, ff_channel, 1),\n                nn.ConvTranspose2d(ff_channel, int(final_dim/2), 4, stride=2, padding=1),\n                nn.BatchNorm2d(int(final_dim/2))          \n            )    \n\n        self.reduction = nn.Conv2d(final_dim, int(final_dim/4), 1)\n        \n        \n    def forward(self, x):\n        b, c, h, w = x.shape\n  \n        x = self.reduction(x)\n        \n        Y1, Yh = xf1(x)\n        Y2, Yh = xf2(x)\n        Y3, Yh = xf3(x)\n        Y4, Yh = xf4(x)\n        \n        x1 = torch.reshape(Yh[0], (b, int(c*3/4), int(h/2), int(w/2)))\n        x2 = torch.reshape(Yh[1], (b, int(c*3/4), int(h/4), int(w/4)))\n        x3 = torch.reshape(Yh[2], (b, int(c*3/4), int(h/8), int(w/8)))\n        x4 = torch.reshape(Yh[3], (b, int(c*3/4), int(h/16), int(w/16)))\n        \n        x1 = torch.cat((Y1,x1), dim = 1)\n        x2 = torch.cat((Y2,x2), dim = 1)\n        x3 = torch.cat((Y3,x3), dim = 1)\n        x4 = torch.cat((Y4,x4), dim = 1)\n        \n        \n        x4 = self.feedforward4(x4)\n        \n        x3 = torch.cat((x3,x4), dim = 1)\n        \n        x3 = self.feedforward3(x3)\n        \n        x2 = torch.cat((x2,x3), dim = 1)\n\n        x2 = self.feedforward2(x2)\n\n        x1 = torch.cat((x1,x2), dim = 1)\n        x = self.feedforward1(x1)\n    \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.218037Z","iopub.execute_input":"2024-05-05T23:02:06.218435Z","iopub.status.idle":"2024-05-05T23:02:06.237093Z","shell.execute_reply.started":"2024-05-05T23:02:06.218403Z","shell.execute_reply":"2024-05-05T23:02:06.236310Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class WaveMix(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_classes=1000,\n        depth = 16,\n        mult = 2,\n        ff_channel = 192,\n        final_dim = 192,\n        dropout = 0.5,\n        level = 3,\n        initial_conv = 'pachify', # or 'strided'\n        patch_size = 4,\n        stride = 2,\n\n    ):\n        super().__init__()\n        \n        self.layers = nn.ModuleList([])\n        for _ in range(depth): \n                if level == 4:\n                    self.layers.append(Level4Waveblock(mult = mult, ff_channel = ff_channel, final_dim = final_dim, dropout = dropout))\n                elif level == 3:\n                    self.layers.append(Level3Waveblock(mult = mult, ff_channel = ff_channel, final_dim = final_dim, dropout = dropout))\n                elif level == 2:\n                    self.layers.append(Level2Waveblock(mult = mult, ff_channel = ff_channel, final_dim = final_dim, dropout = dropout))\n                else:\n                    self.layers.append(Level1Waveblock(mult = mult, ff_channel = ff_channel, final_dim = final_dim, dropout = dropout))\n        \n        self.pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            Rearrange('... () () -> ...'),\n            nn.Linear(final_dim, num_classes)\n        )\n\n        if initial_conv == 'strided':\n            self.conv = nn.Sequential(\n            nn.Conv2d(3, int(final_dim/2), 3, stride, 1),\n            nn.Conv2d(int(final_dim/2), final_dim, 3, stride, 1)\n        )\n        else:\n            self.conv = nn.Sequential(\n            nn.Conv2d(3, int(final_dim/4),3, 1, 1),\n            nn.Conv2d(int(final_dim/4), int(final_dim/2), 3, 1, 1),\n            nn.Conv2d(int(final_dim/2), final_dim, patch_size, patch_size),\n            nn.GELU(),\n            nn.BatchNorm2d(final_dim)\n            )\n        \n\n    def forward(self, img):\n        x = self.conv(img)   \n            \n        for attn in self.layers:\n            x = attn(x) + x\n\n        out = self.pool(x)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.238258Z","iopub.execute_input":"2024-05-05T23:02:06.238885Z","iopub.status.idle":"2024-05-05T23:02:06.252184Z","shell.execute_reply.started":"2024-05-05T23:02:06.238854Z","shell.execute_reply":"2024-05-05T23:02:06.251299Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# https://github.com/pranavphoenix/WaveMix/blob/main/Image_Classification/tinyimagenet.py\nmodel = WaveMix(\n    num_classes = 200,\n    depth = 16,\n    mult = 2,\n    ff_channel = 192,\n    final_dim = 192,\n    dropout = 0.5,\n    level = 3,\n    initial_conv = 'pachify',\n    patch_size = 4\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.253378Z","iopub.execute_input":"2024-05-05T23:02:06.253870Z","iopub.status.idle":"2024-05-05T23:02:06.580501Z","shell.execute_reply.started":"2024-05-05T23:02:06.253846Z","shell.execute_reply":"2024-05-05T23:02:06.579527Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# load pre-trained weights provided by the author\nurl = 'https://huggingface.co/cloudwalker/wavemix/resolve/main/Saved_Models_Weights/tinyimagenet/tiny_71.69.pth'\nmodel.load_state_dict(torch.hub.load_state_dict_from_url(url))\nmodel.to(device)\n\n# summary\nprint(summary(model, (3, 64, 64)))  \n\nPATH = 'tiny_71.69.pth' # path to save the model\n\nprint(\"ImageNet Weights Loaded\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:06.581795Z","iopub.execute_input":"2024-05-05T23:02:06.582180Z","iopub.status.idle":"2024-05-05T23:02:10.494049Z","shell.execute_reply.started":"2024-05-05T23:02:06.582126Z","shell.execute_reply":"2024-05-05T23:02:10.493170Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Downloading: \"https://huggingface.co/cloudwalker/wavemix/resolve/main/Saved_Models_Weights/tinyimagenet/tiny_71.69.pth\" to /root/.cache/torch/hub/checkpoints/tiny_71.69.pth\n100%|██████████| 106M/106M [00:02<00:00, 52.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 48, 64, 64]           1,344\n            Conv2d-2           [-1, 96, 64, 64]          41,568\n            Conv2d-3          [-1, 192, 16, 16]         295,104\n              GELU-4          [-1, 192, 16, 16]               0\n       BatchNorm2d-5          [-1, 192, 16, 16]             384\n            Conv2d-6           [-1, 48, 16, 16]           9,264\n            Conv2d-7            [-1, 384, 2, 2]          74,112\n              GELU-8            [-1, 384, 2, 2]               0\n           Dropout-9            [-1, 384, 2, 2]               0\n           Conv2d-10            [-1, 192, 2, 2]          73,920\n  ConvTranspose2d-11             [-1, 96, 4, 4]         295,008\n      BatchNorm2d-12             [-1, 96, 4, 4]             192\n           Conv2d-13            [-1, 384, 4, 4]         110,976\n             GELU-14            [-1, 384, 4, 4]               0\n          Dropout-15            [-1, 384, 4, 4]               0\n           Conv2d-16            [-1, 192, 4, 4]          73,920\n  ConvTranspose2d-17             [-1, 96, 8, 8]         295,008\n      BatchNorm2d-18             [-1, 96, 8, 8]             192\n           Conv2d-19            [-1, 384, 8, 8]         110,976\n             GELU-20            [-1, 384, 8, 8]               0\n          Dropout-21            [-1, 384, 8, 8]               0\n           Conv2d-22            [-1, 192, 8, 8]          73,920\n  ConvTranspose2d-23          [-1, 192, 16, 16]         590,016\n      BatchNorm2d-24          [-1, 192, 16, 16]             384\n  Level3Waveblock-25          [-1, 192, 16, 16]               0\n           Conv2d-26           [-1, 48, 16, 16]           9,264\n           Conv2d-27            [-1, 384, 2, 2]          74,112\n             GELU-28            [-1, 384, 2, 2]               0\n          Dropout-29            [-1, 384, 2, 2]               0\n           Conv2d-30            [-1, 192, 2, 2]          73,920\n  ConvTranspose2d-31             [-1, 96, 4, 4]         295,008\n      BatchNorm2d-32             [-1, 96, 4, 4]             192\n           Conv2d-33            [-1, 384, 4, 4]         110,976\n             GELU-34            [-1, 384, 4, 4]               0\n          Dropout-35            [-1, 384, 4, 4]               0\n           Conv2d-36            [-1, 192, 4, 4]          73,920\n  ConvTranspose2d-37             [-1, 96, 8, 8]         295,008\n      BatchNorm2d-38             [-1, 96, 8, 8]             192\n           Conv2d-39            [-1, 384, 8, 8]         110,976\n             GELU-40            [-1, 384, 8, 8]               0\n          Dropout-41            [-1, 384, 8, 8]               0\n           Conv2d-42            [-1, 192, 8, 8]          73,920\n  ConvTranspose2d-43          [-1, 192, 16, 16]         590,016\n      BatchNorm2d-44          [-1, 192, 16, 16]             384\n  Level3Waveblock-45          [-1, 192, 16, 16]               0\n           Conv2d-46           [-1, 48, 16, 16]           9,264\n           Conv2d-47            [-1, 384, 2, 2]          74,112\n             GELU-48            [-1, 384, 2, 2]               0\n          Dropout-49            [-1, 384, 2, 2]               0\n           Conv2d-50            [-1, 192, 2, 2]          73,920\n  ConvTranspose2d-51             [-1, 96, 4, 4]         295,008\n      BatchNorm2d-52             [-1, 96, 4, 4]             192\n           Conv2d-53            [-1, 384, 4, 4]         110,976\n             GELU-54            [-1, 384, 4, 4]               0\n          Dropout-55            [-1, 384, 4, 4]               0\n           Conv2d-56            [-1, 192, 4, 4]          73,920\n  ConvTranspose2d-57             [-1, 96, 8, 8]         295,008\n      BatchNorm2d-58             [-1, 96, 8, 8]             192\n           Conv2d-59            [-1, 384, 8, 8]         110,976\n             GELU-60            [-1, 384, 8, 8]               0\n          Dropout-61            [-1, 384, 8, 8]               0\n           Conv2d-62            [-1, 192, 8, 8]          73,920\n  ConvTranspose2d-63          [-1, 192, 16, 16]         590,016\n      BatchNorm2d-64          [-1, 192, 16, 16]             384\n  Level3Waveblock-65          [-1, 192, 16, 16]               0\n           Conv2d-66           [-1, 48, 16, 16]           9,264\n           Conv2d-67            [-1, 384, 2, 2]          74,112\n             GELU-68            [-1, 384, 2, 2]               0\n          Dropout-69            [-1, 384, 2, 2]               0\n           Conv2d-70            [-1, 192, 2, 2]          73,920\n  ConvTranspose2d-71             [-1, 96, 4, 4]         295,008\n      BatchNorm2d-72             [-1, 96, 4, 4]             192\n           Conv2d-73            [-1, 384, 4, 4]         110,976\n             GELU-74            [-1, 384, 4, 4]               0\n          Dropout-75            [-1, 384, 4, 4]               0\n           Conv2d-76            [-1, 192, 4, 4]          73,920\n  ConvTranspose2d-77             [-1, 96, 8, 8]         295,008\n      BatchNorm2d-78             [-1, 96, 8, 8]             192\n           Conv2d-79            [-1, 384, 8, 8]         110,976\n             GELU-80            [-1, 384, 8, 8]               0\n          Dropout-81            [-1, 384, 8, 8]               0\n           Conv2d-82            [-1, 192, 8, 8]          73,920\n  ConvTranspose2d-83          [-1, 192, 16, 16]         590,016\n      BatchNorm2d-84          [-1, 192, 16, 16]             384\n  Level3Waveblock-85          [-1, 192, 16, 16]               0\n           Conv2d-86           [-1, 48, 16, 16]           9,264\n           Conv2d-87            [-1, 384, 2, 2]          74,112\n             GELU-88            [-1, 384, 2, 2]               0\n          Dropout-89            [-1, 384, 2, 2]               0\n           Conv2d-90            [-1, 192, 2, 2]          73,920\n  ConvTranspose2d-91             [-1, 96, 4, 4]         295,008\n      BatchNorm2d-92             [-1, 96, 4, 4]             192\n           Conv2d-93            [-1, 384, 4, 4]         110,976\n             GELU-94            [-1, 384, 4, 4]               0\n          Dropout-95            [-1, 384, 4, 4]               0\n           Conv2d-96            [-1, 192, 4, 4]          73,920\n  ConvTranspose2d-97             [-1, 96, 8, 8]         295,008\n      BatchNorm2d-98             [-1, 96, 8, 8]             192\n           Conv2d-99            [-1, 384, 8, 8]         110,976\n            GELU-100            [-1, 384, 8, 8]               0\n         Dropout-101            [-1, 384, 8, 8]               0\n          Conv2d-102            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-103          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-104          [-1, 192, 16, 16]             384\n Level3Waveblock-105          [-1, 192, 16, 16]               0\n          Conv2d-106           [-1, 48, 16, 16]           9,264\n          Conv2d-107            [-1, 384, 2, 2]          74,112\n            GELU-108            [-1, 384, 2, 2]               0\n         Dropout-109            [-1, 384, 2, 2]               0\n          Conv2d-110            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-111             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-112             [-1, 96, 4, 4]             192\n          Conv2d-113            [-1, 384, 4, 4]         110,976\n            GELU-114            [-1, 384, 4, 4]               0\n         Dropout-115            [-1, 384, 4, 4]               0\n          Conv2d-116            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-117             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-118             [-1, 96, 8, 8]             192\n          Conv2d-119            [-1, 384, 8, 8]         110,976\n            GELU-120            [-1, 384, 8, 8]               0\n         Dropout-121            [-1, 384, 8, 8]               0\n          Conv2d-122            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-123          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-124          [-1, 192, 16, 16]             384\n Level3Waveblock-125          [-1, 192, 16, 16]               0\n          Conv2d-126           [-1, 48, 16, 16]           9,264\n          Conv2d-127            [-1, 384, 2, 2]          74,112\n            GELU-128            [-1, 384, 2, 2]               0\n         Dropout-129            [-1, 384, 2, 2]               0\n          Conv2d-130            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-131             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-132             [-1, 96, 4, 4]             192\n          Conv2d-133            [-1, 384, 4, 4]         110,976\n            GELU-134            [-1, 384, 4, 4]               0\n         Dropout-135            [-1, 384, 4, 4]               0\n          Conv2d-136            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-137             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-138             [-1, 96, 8, 8]             192\n          Conv2d-139            [-1, 384, 8, 8]         110,976\n            GELU-140            [-1, 384, 8, 8]               0\n         Dropout-141            [-1, 384, 8, 8]               0\n          Conv2d-142            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-143          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-144          [-1, 192, 16, 16]             384\n Level3Waveblock-145          [-1, 192, 16, 16]               0\n          Conv2d-146           [-1, 48, 16, 16]           9,264\n          Conv2d-147            [-1, 384, 2, 2]          74,112\n            GELU-148            [-1, 384, 2, 2]               0\n         Dropout-149            [-1, 384, 2, 2]               0\n          Conv2d-150            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-151             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-152             [-1, 96, 4, 4]             192\n          Conv2d-153            [-1, 384, 4, 4]         110,976\n            GELU-154            [-1, 384, 4, 4]               0\n         Dropout-155            [-1, 384, 4, 4]               0\n          Conv2d-156            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-157             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-158             [-1, 96, 8, 8]             192\n          Conv2d-159            [-1, 384, 8, 8]         110,976\n            GELU-160            [-1, 384, 8, 8]               0\n         Dropout-161            [-1, 384, 8, 8]               0\n          Conv2d-162            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-163          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-164          [-1, 192, 16, 16]             384\n Level3Waveblock-165          [-1, 192, 16, 16]               0\n          Conv2d-166           [-1, 48, 16, 16]           9,264\n          Conv2d-167            [-1, 384, 2, 2]          74,112\n            GELU-168            [-1, 384, 2, 2]               0\n         Dropout-169            [-1, 384, 2, 2]               0\n          Conv2d-170            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-171             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-172             [-1, 96, 4, 4]             192\n          Conv2d-173            [-1, 384, 4, 4]         110,976\n            GELU-174            [-1, 384, 4, 4]               0\n         Dropout-175            [-1, 384, 4, 4]               0\n          Conv2d-176            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-177             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-178             [-1, 96, 8, 8]             192\n          Conv2d-179            [-1, 384, 8, 8]         110,976\n            GELU-180            [-1, 384, 8, 8]               0\n         Dropout-181            [-1, 384, 8, 8]               0\n          Conv2d-182            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-183          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-184          [-1, 192, 16, 16]             384\n Level3Waveblock-185          [-1, 192, 16, 16]               0\n          Conv2d-186           [-1, 48, 16, 16]           9,264\n          Conv2d-187            [-1, 384, 2, 2]          74,112\n            GELU-188            [-1, 384, 2, 2]               0\n         Dropout-189            [-1, 384, 2, 2]               0\n          Conv2d-190            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-191             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-192             [-1, 96, 4, 4]             192\n          Conv2d-193            [-1, 384, 4, 4]         110,976\n            GELU-194            [-1, 384, 4, 4]               0\n         Dropout-195            [-1, 384, 4, 4]               0\n          Conv2d-196            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-197             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-198             [-1, 96, 8, 8]             192\n          Conv2d-199            [-1, 384, 8, 8]         110,976\n            GELU-200            [-1, 384, 8, 8]               0\n         Dropout-201            [-1, 384, 8, 8]               0\n          Conv2d-202            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-203          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-204          [-1, 192, 16, 16]             384\n Level3Waveblock-205          [-1, 192, 16, 16]               0\n          Conv2d-206           [-1, 48, 16, 16]           9,264\n          Conv2d-207            [-1, 384, 2, 2]          74,112\n            GELU-208            [-1, 384, 2, 2]               0\n         Dropout-209            [-1, 384, 2, 2]               0\n          Conv2d-210            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-211             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-212             [-1, 96, 4, 4]             192\n          Conv2d-213            [-1, 384, 4, 4]         110,976\n            GELU-214            [-1, 384, 4, 4]               0\n         Dropout-215            [-1, 384, 4, 4]               0\n          Conv2d-216            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-217             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-218             [-1, 96, 8, 8]             192\n          Conv2d-219            [-1, 384, 8, 8]         110,976\n            GELU-220            [-1, 384, 8, 8]               0\n         Dropout-221            [-1, 384, 8, 8]               0\n          Conv2d-222            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-223          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-224          [-1, 192, 16, 16]             384\n Level3Waveblock-225          [-1, 192, 16, 16]               0\n          Conv2d-226           [-1, 48, 16, 16]           9,264\n          Conv2d-227            [-1, 384, 2, 2]          74,112\n            GELU-228            [-1, 384, 2, 2]               0\n         Dropout-229            [-1, 384, 2, 2]               0\n          Conv2d-230            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-231             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-232             [-1, 96, 4, 4]             192\n          Conv2d-233            [-1, 384, 4, 4]         110,976\n            GELU-234            [-1, 384, 4, 4]               0\n         Dropout-235            [-1, 384, 4, 4]               0\n          Conv2d-236            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-237             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-238             [-1, 96, 8, 8]             192\n          Conv2d-239            [-1, 384, 8, 8]         110,976\n            GELU-240            [-1, 384, 8, 8]               0\n         Dropout-241            [-1, 384, 8, 8]               0\n          Conv2d-242            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-243          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-244          [-1, 192, 16, 16]             384\n Level3Waveblock-245          [-1, 192, 16, 16]               0\n          Conv2d-246           [-1, 48, 16, 16]           9,264\n          Conv2d-247            [-1, 384, 2, 2]          74,112\n            GELU-248            [-1, 384, 2, 2]               0\n         Dropout-249            [-1, 384, 2, 2]               0\n          Conv2d-250            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-251             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-252             [-1, 96, 4, 4]             192\n          Conv2d-253            [-1, 384, 4, 4]         110,976\n            GELU-254            [-1, 384, 4, 4]               0\n         Dropout-255            [-1, 384, 4, 4]               0\n          Conv2d-256            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-257             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-258             [-1, 96, 8, 8]             192\n          Conv2d-259            [-1, 384, 8, 8]         110,976\n            GELU-260            [-1, 384, 8, 8]               0\n         Dropout-261            [-1, 384, 8, 8]               0\n          Conv2d-262            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-263          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-264          [-1, 192, 16, 16]             384\n Level3Waveblock-265          [-1, 192, 16, 16]               0\n          Conv2d-266           [-1, 48, 16, 16]           9,264\n          Conv2d-267            [-1, 384, 2, 2]          74,112\n            GELU-268            [-1, 384, 2, 2]               0\n         Dropout-269            [-1, 384, 2, 2]               0\n          Conv2d-270            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-271             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-272             [-1, 96, 4, 4]             192\n          Conv2d-273            [-1, 384, 4, 4]         110,976\n            GELU-274            [-1, 384, 4, 4]               0\n         Dropout-275            [-1, 384, 4, 4]               0\n          Conv2d-276            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-277             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-278             [-1, 96, 8, 8]             192\n          Conv2d-279            [-1, 384, 8, 8]         110,976\n            GELU-280            [-1, 384, 8, 8]               0\n         Dropout-281            [-1, 384, 8, 8]               0\n          Conv2d-282            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-283          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-284          [-1, 192, 16, 16]             384\n Level3Waveblock-285          [-1, 192, 16, 16]               0\n          Conv2d-286           [-1, 48, 16, 16]           9,264\n          Conv2d-287            [-1, 384, 2, 2]          74,112\n            GELU-288            [-1, 384, 2, 2]               0\n         Dropout-289            [-1, 384, 2, 2]               0\n          Conv2d-290            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-291             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-292             [-1, 96, 4, 4]             192\n          Conv2d-293            [-1, 384, 4, 4]         110,976\n            GELU-294            [-1, 384, 4, 4]               0\n         Dropout-295            [-1, 384, 4, 4]               0\n          Conv2d-296            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-297             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-298             [-1, 96, 8, 8]             192\n          Conv2d-299            [-1, 384, 8, 8]         110,976\n            GELU-300            [-1, 384, 8, 8]               0\n         Dropout-301            [-1, 384, 8, 8]               0\n          Conv2d-302            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-303          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-304          [-1, 192, 16, 16]             384\n Level3Waveblock-305          [-1, 192, 16, 16]               0\n          Conv2d-306           [-1, 48, 16, 16]           9,264\n          Conv2d-307            [-1, 384, 2, 2]          74,112\n            GELU-308            [-1, 384, 2, 2]               0\n         Dropout-309            [-1, 384, 2, 2]               0\n          Conv2d-310            [-1, 192, 2, 2]          73,920\n ConvTranspose2d-311             [-1, 96, 4, 4]         295,008\n     BatchNorm2d-312             [-1, 96, 4, 4]             192\n          Conv2d-313            [-1, 384, 4, 4]         110,976\n            GELU-314            [-1, 384, 4, 4]               0\n         Dropout-315            [-1, 384, 4, 4]               0\n          Conv2d-316            [-1, 192, 4, 4]          73,920\n ConvTranspose2d-317             [-1, 96, 8, 8]         295,008\n     BatchNorm2d-318             [-1, 96, 8, 8]             192\n          Conv2d-319            [-1, 384, 8, 8]         110,976\n            GELU-320            [-1, 384, 8, 8]               0\n         Dropout-321            [-1, 384, 8, 8]               0\n          Conv2d-322            [-1, 192, 8, 8]          73,920\n ConvTranspose2d-323          [-1, 192, 16, 16]         590,016\n     BatchNorm2d-324          [-1, 192, 16, 16]             384\n Level3Waveblock-325          [-1, 192, 16, 16]               0\nAdaptiveAvgPool2d-326            [-1, 192, 1, 1]               0\n       Rearrange-327                  [-1, 192]               0\n          Linear-328                  [-1, 200]          38,600\n================================================================\nTotal params: 27,703,208\nTrainable params: 27,703,208\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 40.79\nParams size (MB): 105.68\nEstimated Total Size (MB): 146.51\n----------------------------------------------------------------\nNone\nImageNet Weights Loaded\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"counter = 3 # number of epochs without any improvement in accuracy before the training stops for each optimiser\nnum_classes = 200","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:10.495326Z","iopub.execute_input":"2024-05-05T23:02:10.495620Z","iopub.status.idle":"2024-05-05T23:02:10.499965Z","shell.execute_reply.started":"2024-05-05T23:02:10.495595Z","shell.execute_reply":"2024-05-05T23:02:10.498889Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Baseline","metadata":{}},{"cell_type":"code","source":"top1 = [] # top1 accuracy\ntop5 = [] # top5 accuracy\ntraintime = []\ntesttime = []","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:10.501156Z","iopub.execute_input":"2024-05-05T23:02:10.501428Z","iopub.status.idle":"2024-05-05T23:02:10.537634Z","shell.execute_reply.started":"2024-05-05T23:02:10.501404Z","shell.execute_reply":"2024-05-05T23:02:10.536930Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# iterative deepening dual-optimiser training\nclassification(model, trainloader_1, testloader, device, PATH, top1, top5, traintime, testtime, num_classes=num_classes, set_counter=counter)\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:02:10.538716Z","iopub.execute_input":"2024-05-05T23:02:10.538982Z","iopub.status.idle":"2024-05-06T00:38:44.617656Z","shell.execute_reply.started":"2024-05-05T23:02:10.538951Z","shell.execute_reply":"2024-05-06T00:38:44.616545Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Training with AdamW\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.2392 - acc: 0.6721]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 70.45 - Top 5: 89.34 -  Train Time: 278.91 - Test Time: 13.04\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.2317 - acc: 0.6722]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 69.92 - Top 5: 89.50 -  Train Time: 278.37 - Test Time: 12.79\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.2438 - acc: 0.6727]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 69.98 - Top 5: 89.46 -  Train Time: 278.30 - Test Time: 12.80\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.2139 - acc: 0.6770]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 4 - Top 1: 70.62 - Top 5: 89.39 -  Train Time: 278.19 - Test Time: 12.78\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.2124 - acc: 0.6788]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 5 - Top 1: 69.83 - Top 5: 89.50 -  Train Time: 278.13 - Test Time: 12.79\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.2063 - acc: 0.6789]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 6 - Top 1: 70.44 - Top 5: 89.27 -  Train Time: 278.43 - Test Time: 12.82\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.1859 - acc: 0.6834]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 7 - Top 1: 69.99 - Top 5: 89.11 -  Train Time: 278.37 - Test Time: 12.90\n\nFinished Training\nTraining with SGD\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1587 - acc: 0.6931]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 71.04 - Top 5: 89.70 -  Train Time: 276.20 - Test Time: 12.80\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1424 - acc: 0.6957]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 71.03 - Top 5: 89.67 -  Train Time: 276.13 - Test Time: 12.80\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1356 - acc: 0.6993]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 71.17 - Top 5: 89.77 -  Train Time: 276.11 - Test Time: 12.81\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1384 - acc: 0.6981]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 4 - Top 1: 71.06 - Top 5: 89.74 -  Train Time: 275.99 - Test Time: 12.89\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1369 - acc: 0.6969]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 5 - Top 1: 71.21 - Top 5: 89.71 -  Train Time: 276.03 - Test Time: 12.88\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1335 - acc: 0.6988]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 6 - Top 1: 71.49 - Top 5: 89.93 -  Train Time: 275.78 - Test Time: 12.77\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1321 - acc: 0.6996]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 7 - Top 1: 71.57 - Top 5: 89.93 -  Train Time: 275.56 - Test Time: 12.85\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1303 - acc: 0.6992]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 8 - Top 1: 71.58 - Top 5: 89.89 -  Train Time: 275.63 - Test Time: 12.84\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1311 - acc: 0.6993]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 9 - Top 1: 71.62 - Top 5: 89.80 -  Train Time: 275.70 - Test Time: 12.90\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1285 - acc: 0.6995]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 10 - Top 1: 71.66 - Top 5: 89.85 -  Train Time: 275.70 - Test Time: 12.77\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1227 - acc: 0.7017]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 11 - Top 1: 71.52 - Top 5: 89.89 -  Train Time: 275.63 - Test Time: 12.85\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1200 - acc: 0.7007]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 12 - Top 1: 71.56 - Top 5: 89.81 -  Train Time: 275.66 - Test Time: 12.76\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1215 - acc: 0.7018]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 13 - Top 1: 71.36 - Top 5: 89.81 -  Train Time: 275.69 - Test Time: 12.76\n\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:38:44.624350Z","iopub.execute_input":"2024-05-06T00:38:44.624651Z","iopub.status.idle":"2024-05-06T00:38:44.784260Z","shell.execute_reply.started":"2024-05-06T00:38:44.624617Z","shell.execute_reply":"2024-05-06T00:38:44.783292Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"post_train(model, trainloader_2, testloader, device, PATH, top1, top5, traintime, testtime, num_classes=num_classes, set_counter=counter)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:38:44.785343Z","iopub.execute_input":"2024-05-06T00:38:44.785623Z","iopub.status.idle":"2024-05-06T00:53:10.123359Z","shell.execute_reply.started":"2024-05-06T00:38:44.785599Z","shell.execute_reply":"2024-05-06T00:53:10.122316Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Post-training with SGD\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 0.6575 - acc: 0.8136]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 71.27 - Top 5: 89.80 -  Train Time: 275.70 - Test Time: 12.87\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 0.6467 - acc: 0.8160]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 71.40 - Top 5: 89.86 -  Train Time: 275.60 - Test Time: 12.81\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 0.6436 - acc: 0.8176]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 71.34 - Top 5: 89.75 -  Train Time: 275.58 - Test Time: 12.78\n\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Results\")\nprint(f\"Top 1 Accuracy: {max(top1):.2f} -Top 5 Accuracy : {max(top5):.2f} - Train Time: {min(traintime):.0f} -Test Time: {min(testtime):.0f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:53:10.124910Z","iopub.execute_input":"2024-05-06T00:53:10.125267Z","iopub.status.idle":"2024-05-06T00:53:10.132260Z","shell.execute_reply.started":"2024-05-06T00:53:10.125227Z","shell.execute_reply":"2024-05-06T00:53:10.131376Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Results\nTop 1 Accuracy: 71.66 -Top 5 Accuracy : 89.93 - Train Time: 276 -Test Time: 13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Blurred validation images","metadata":{}},{"cell_type":"code","source":"top1 = [] # top1 accuracy\ntop5 = [] # top5 accuracy\ntraintime = []\ntesttime = []","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:53:10.133434Z","iopub.execute_input":"2024-05-06T00:53:10.133701Z","iopub.status.idle":"2024-05-06T00:53:10.142902Z","shell.execute_reply.started":"2024-05-06T00:53:10.133678Z","shell.execute_reply":"2024-05-06T00:53:10.142191Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"classification(model, trainloader_blur_1, testloader_blur, device, PATH, top1, top5, traintime, testtime, num_classes=num_classes, set_counter=counter)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T00:53:10.144029Z","iopub.execute_input":"2024-05-06T00:53:10.144609Z","iopub.status.idle":"2024-05-06T01:51:09.198240Z","shell.execute_reply.started":"2024-05-06T00:53:10.144571Z","shell.execute_reply":"2024-05-06T01:51:09.197032Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Training with AdamW\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.2111 - acc: 0.6789]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 51.35 - Top 5: 77.03 -  Train Time: 278.17 - Test Time: 13.14\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.1981 - acc: 0.6800]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 51.60 - Top 5: 76.80 -  Train Time: 278.06 - Test Time: 12.77\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.1893 - acc: 0.6842]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 52.23 - Top 5: 77.59 -  Train Time: 277.99 - Test Time: 12.78\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.1763 - acc: 0.6866]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 4 - Top 1: 52.06 - Top 5: 77.40 -  Train Time: 277.96 - Test Time: 12.78\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.1712 - acc: 0.6880]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 5 - Top 1: 50.09 - Top 5: 75.61 -  Train Time: 277.91 - Test Time: 12.84\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 329/329 [04:37<00:00,  1.18batch/s,  loss : 1.1549 - acc: 0.6898]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 6 - Top 1: 51.63 - Top 5: 77.01 -  Train Time: 277.99 - Test Time: 12.80\n\nFinished Training\nTraining with SGD\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1303 - acc: 0.6991]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 52.10 - Top 5: 77.07 -  Train Time: 275.70 - Test Time: 12.86\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1109 - acc: 0.7031]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 52.57 - Top 5: 77.29 -  Train Time: 275.69 - Test Time: 12.88\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1094 - acc: 0.7048]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 52.63 - Top 5: 77.58 -  Train Time: 275.97 - Test Time: 12.90\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1075 - acc: 0.7049]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 4 - Top 1: 51.37 - Top 5: 76.53 -  Train Time: 275.81 - Test Time: 12.87\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1038 - acc: 0.7051]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 5 - Top 1: 52.13 - Top 5: 77.16 -  Train Time: 275.84 - Test Time: 12.79\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 329/329 [04:35<00:00,  1.19batch/s,  loss : 1.1033 - acc: 0.7070]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 6 - Top 1: 51.86 - Top 5: 76.89 -  Train Time: 275.86 - Test Time: 12.92\n\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:51:09.199965Z","iopub.execute_input":"2024-05-06T01:51:09.200892Z","iopub.status.idle":"2024-05-06T01:51:09.359605Z","shell.execute_reply.started":"2024-05-06T01:51:09.200845Z","shell.execute_reply":"2024-05-06T01:51:09.358725Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"post_train(model, trainloader_blur_2, testloader_blur, device, PATH, top1, top5, traintime, testtime, num_classes=num_classes, set_counter=counter)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T01:51:09.360871Z","iopub.execute_input":"2024-05-06T01:51:09.361253Z","iopub.status.idle":"2024-05-06T02:05:37.412613Z","shell.execute_reply.started":"2024-05-06T01:51:09.361219Z","shell.execute_reply":"2024-05-06T02:05:37.411448Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Post-training with SGD\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 0.6303 - acc: 0.8215]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 51.25 - Top 5: 76.55 -  Train Time: 276.55 - Test Time: 12.91\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 0.6171 - acc: 0.8251]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 50.88 - Top 5: 76.49 -  Train Time: 276.38 - Test Time: 12.93\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 0.6111 - acc: 0.8266]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 50.51 - Top 5: 75.95 -  Train Time: 276.36 - Test Time: 12.91\n\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Results\")\nprint(f\"Top 1 Accuracy: {max(top1):.2f} -Top 5 Accuracy : {max(top5):.2f} - Train Time: {min(traintime):.0f} -Test Time: {min(testtime):.0f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:05:37.414064Z","iopub.execute_input":"2024-05-06T02:05:37.414401Z","iopub.status.idle":"2024-05-06T02:05:37.423719Z","shell.execute_reply.started":"2024-05-06T02:05:37.414369Z","shell.execute_reply":"2024-05-06T02:05:37.422633Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Results\nTop 1 Accuracy: 52.63 -Top 5 Accuracy : 77.59 - Train Time: 276 -Test Time: 13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Validation images with Gaussian noises","metadata":{}},{"cell_type":"code","source":"top1 = [] # top1 accuracy\ntop5 = [] # top5 accuracy\ntraintime = []\ntesttime = []","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:05:37.425073Z","iopub.execute_input":"2024-05-06T02:05:37.425449Z","iopub.status.idle":"2024-05-06T02:05:37.438762Z","shell.execute_reply.started":"2024-05-06T02:05:37.425417Z","shell.execute_reply":"2024-05-06T02:05:37.437805Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"classification(model, trainloader_noise_1, testloader_noise, device, PATH, top1, top5, traintime, testtime, num_classes=num_classes, set_counter=counter)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:05:37.439942Z","iopub.execute_input":"2024-05-06T02:05:37.440365Z","iopub.status.idle":"2024-05-06T02:49:19.086338Z","shell.execute_reply.started":"2024-05-06T02:05:37.440335Z","shell.execute_reply":"2024-05-06T02:49:19.084995Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Training with AdamW\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:39<00:00,  1.18batch/s,  loss : 1.1752 - acc: 0.6885]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 43.22 - Top 5: 68.71 -  Train Time: 279.34 - Test Time: 13.02\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.1726 - acc: 0.6875]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 43.22 - Top 5: 67.36 -  Train Time: 279.14 - Test Time: 12.85\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.1557 - acc: 0.6935]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 44.57 - Top 5: 69.53 -  Train Time: 278.92 - Test Time: 12.90\n\n1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.1560 - acc: 0.6928]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 4 - Top 1: 42.50 - Top 5: 67.55 -  Train Time: 279.17 - Test Time: 12.85\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.1325 - acc: 0.6970]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 5 - Top 1: 44.47 - Top 5: 69.24 -  Train Time: 278.91 - Test Time: 12.92\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 329/329 [04:38<00:00,  1.18batch/s,  loss : 1.1333 - acc: 0.6975]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 6 - Top 1: 43.83 - Top 5: 68.81 -  Train Time: 278.91 - Test Time: 12.92\n\nFinished Training\nTraining with SGD\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 1.1004 - acc: 0.7060]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 44.05 - Top 5: 68.67 -  Train Time: 276.59 - Test Time: 12.95\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 1.0836 - acc: 0.7096]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 44.21 - Top 5: 69.24 -  Train Time: 276.68 - Test Time: 12.86\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 1.0824 - acc: 0.7115]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 44.38 - Top 5: 69.23 -  Train Time: 276.59 - Test Time: 12.92\n\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:49:19.087867Z","iopub.execute_input":"2024-05-06T02:49:19.088201Z","iopub.status.idle":"2024-05-06T02:49:19.271904Z","shell.execute_reply.started":"2024-05-06T02:49:19.088165Z","shell.execute_reply":"2024-05-06T02:49:19.270967Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"post_train(model, trainloader_noise_2, testloader_noise, device, PATH, top1, top5, traintime, testtime, num_classes=num_classes, set_counter=counter)\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:49:19.273122Z","iopub.execute_input":"2024-05-06T02:49:19.273439Z","iopub.status.idle":"2024-05-06T03:03:47.434608Z","shell.execute_reply.started":"2024-05-06T02:49:19.273415Z","shell.execute_reply":"2024-05-06T03:03:47.433468Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Post-training with SGD\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 0.6149 - acc: 0.8236]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 - Top 1: 43.70 - Top 5: 68.83 -  Train Time: 276.71 - Test Time: 12.82\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 0.5964 - acc: 0.8291]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 - Top 1: 44.35 - Top 5: 69.31 -  Train Time: 276.38 - Test Time: 12.92\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [04:36<00:00,  1.19batch/s,  loss : 0.5911 - acc: 0.8315]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 - Top 1: 44.27 - Top 5: 69.24 -  Train Time: 276.39 - Test Time: 12.92\n\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Results\")\nprint(f\"Top 1 Accuracy: {max(top1):.2f} -Top 5 Accuracy : {max(top5):.2f} - Train Time: {min(traintime):.0f} -Test Time: {min(testtime):.0f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:03:47.436278Z","iopub.execute_input":"2024-05-06T03:03:47.437249Z","iopub.status.idle":"2024-05-06T03:03:47.443701Z","shell.execute_reply.started":"2024-05-06T03:03:47.437208Z","shell.execute_reply":"2024-05-06T03:03:47.442847Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Results\nTop 1 Accuracy: 44.57 -Top 5 Accuracy : 69.53 - Train Time: 276 -Test Time: 13\n","output_type":"stream"}]}]}